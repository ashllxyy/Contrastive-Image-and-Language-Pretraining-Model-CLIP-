{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install img2dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-22T23:21:45.280265Z","iopub.execute_input":"2023-07-22T23:21:45.281097Z","iopub.status.idle":"2023-07-22T23:22:18.218411Z","shell.execute_reply.started":"2023-07-22T23:21:45.281062Z","shell.execute_reply":"2023-07-22T23:22:18.217251Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting img2dataset\n  Downloading img2dataset-1.41.0-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm<5,>=4.62.3 in /opt/conda/lib/python3.10/site-packages (from img2dataset) (4.65.0)\nRequirement already satisfied: opencv-python-headless<5,>=4.5.5.62 in /opt/conda/lib/python3.10/site-packages (from img2dataset) (4.8.0.74)\nCollecting fire<0.5.0,>=0.4.0 (from img2dataset)\n  Downloading fire-0.4.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting webdataset<0.3,>=0.2.5 (from img2dataset)\n  Downloading webdataset-0.2.48-py3-none-any.whl (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas<2,>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from img2dataset) (1.5.3)\nCollecting pyarrow<8,>=6.0.1 (from img2dataset)\n  Downloading pyarrow-7.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting exifread-nocycle<4,>=3.0.1 (from img2dataset)\n  Downloading ExifRead_nocycle-3.0.1-py3-none-any.whl (39 kB)\nRequirement already satisfied: albumentations<2,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from img2dataset) (1.3.1)\nCollecting dataclasses<1.0.0,>=0.6 (from img2dataset)\n  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\nCollecting wandb<0.13,>=0.12.10 (from img2dataset)\n  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec==2022.11.0 (from img2dataset)\n  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.10/site-packages (from albumentations<2,>=1.1.0->img2dataset) (1.23.5)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations<2,>=1.1.0->img2dataset) (1.11.1)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations<2,>=1.1.0->img2dataset) (0.21.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations<2,>=1.1.0->img2dataset) (6.0)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations<2,>=1.1.0->img2dataset) (0.0.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire<0.5.0,>=0.4.0->img2dataset) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire<0.5.0,>=0.4.0->img2dataset) (2.3.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2,>=1.1.5->img2dataset) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2,>=1.1.5->img2dataset) (2023.3)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (8.1.3)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (2.31.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (2.3)\nCollecting shortuuid>=0.5.0 (from wandb<0.13,>=0.12.10->img2dataset)\n  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (0.4.0)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (3.20.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb<0.13,>=0.12.10->img2dataset) (59.8.0)\nCollecting braceexpand (from webdataset<0.3,>=0.2.5->img2dataset)\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb<0.13,>=0.12.10->img2dataset) (4.0.10)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb<0.13,>=0.12.10->img2dataset) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb<0.13,>=0.12.10->img2dataset) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb<0.13,>=0.12.10->img2dataset) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb<0.13,>=0.12.10->img2dataset) (2023.5.7)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (0.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13,>=0.12.10->img2dataset) (5.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (3.0.9)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (3.1.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=9e447d45626565bfda5d32af9b92ff67575b9f3883ce0c2f62edbf3ad3efcb76\n  Stored in directory: /root/.cache/pip/wheels/26/9a/dd/2818b1b023daf077ec3e625c47ae446aca587a5abe48e05212\nSuccessfully built fire\nInstalling collected packages: exifread-nocycle, dataclasses, braceexpand, webdataset, shortuuid, pyarrow, fsspec, fire, wandb, img2dataset\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.6.0\n    Uninstalling fsspec-2023.6.0:\n      Successfully uninstalled fsspec-2023.6.0\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.6.1 requires pyarrow==11.*, but you have pyarrow 7.0.0 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2022.11.0 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ns3fs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2022.11.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed braceexpand-0.1.7 dataclasses-0.6 exifread-nocycle-3.0.1 fire-0.4.0 fsspec-2022.11.0 img2dataset-1.41.0 pyarrow-7.0.0 shortuuid-1.0.11 wandb-0.12.21 webdataset-0.2.48\n","output_type":"stream"}]},{"cell_type":"code","source":"path_json = '../input/guie-laion5b-dataset/GUIE_laion5b_dataset_en.json'\n!ls $path_json","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:22:18.221459Z","iopub.execute_input":"2023-07-22T23:22:18.221830Z","iopub.status.idle":"2023-07-22T23:22:19.169609Z","shell.execute_reply.started":"2023-07-22T23:22:18.221793Z","shell.execute_reply":"2023-07-22T23:22:19.168453Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"../input/guie-laion5b-dataset/GUIE_laion5b_dataset_en.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:22:19.171275Z","iopub.execute_input":"2023-07-22T23:22:19.172187Z","iopub.status.idle":"2023-07-22T23:22:30.586020Z","shell.execute_reply.started":"2023-07-22T23:22:19.172146Z","shell.execute_reply":"2023-07-22T23:22:30.584815Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.12.21)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.0.11)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Dict, List, Callable\nfrom pathlib import Path\nfrom PIL import Image\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport torch.optim as optim\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport numpy as np\nimport wandb\nfrom pytorch_lightning.loggers.wandb import WandbLogger\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:22:30.590636Z","iopub.execute_input":"2023-07-22T23:22:30.590959Z","iopub.status.idle":"2023-07-22T23:22:43.566489Z","shell.execute_reply.started":"2023-07-22T23:22:30.590929Z","shell.execute_reply":"2023-07-22T23:22:43.565455Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:22:43.567769Z","iopub.execute_input":"2023-07-22T23:22:43.568128Z","iopub.status.idle":"2023-07-22T23:23:33.875406Z","shell.execute_reply.started":"2023-07-22T23:22:43.568092Z","shell.execute_reply":"2023-07-22T23:23:33.874379Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.login()  # To confirm login","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:23:33.876958Z","iopub.execute_input":"2023-07-22T23:23:33.877274Z","iopub.status.idle":"2023-07-22T23:23:33.888854Z","shell.execute_reply.started":"2023-07-22T23:23:33.877248Z","shell.execute_reply":"2023-07-22T23:23:33.887950Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashllxyy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#DataConfig\nnum_pairs_taken = 20_000 # Number of pairs to download/build vocabulary on\nmax_seq_length = 75 # Maximum sequence length for tokenized sentences","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:23:33.890227Z","iopub.execute_input":"2023-07-22T23:23:33.890800Z","iopub.status.idle":"2023-07-22T23:23:33.899555Z","shell.execute_reply.started":"2023-07-22T23:23:33.890766Z","shell.execute_reply":"2023-07-22T23:23:33.898536Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(path_json)\ndf['text'] = df['caption']\ndf['text_en'] = df['caption_en']\n\ndf.drop(columns=['caption', 'caption_en'], inplace=True)\n\ndf_head = df.head(num_pairs_taken)\ndf_head.to_json('./fixed_df.json', orient='records')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:23:33.900956Z","iopub.execute_input":"2023-07-22T23:23:33.901553Z","iopub.status.idle":"2023-07-22T23:23:39.343080Z","shell.execute_reply.started":"2023-07-22T23:23:33.901520Z","shell.execute_reply":"2023-07-22T23:23:39.342082Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"out_dir = Path('./dataset')\nout_dir.mkdir(exist_ok=True)\n!img2dataset \"fixed_df.json\" --input_format=\"json\" --caption_col=\"text_en\" --output_folder=$out_dir --processes_count=1 --output_format=\"files\" --resize_mode=\"no\"","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:23:39.344759Z","iopub.execute_input":"2023-07-22T23:23:39.345142Z","iopub.status.idle":"2023-07-22T23:32:44.406291Z","shell.execute_reply.started":"2023-07-22T23:23:39.345103Z","shell.execute_reply":"2023-07-22T23:32:44.405037Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nStarting the downloading of this file\nSharding file number 1 of 1 called /kaggle/working/fixed_df.json\n0it [00:00, ?it/s]File sharded in 2 shards\nDownloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n1it [04:41, 281.27s/it]worker  - success: 0.872 - failed to download: 0.115 - failed to resize: 0.014 - images per sec: 36 - count: 10000\ntotal   - success: 0.872 - failed to download: 0.115 - failed to resize: 0.014 - images per sec: 36 - count: 10000\n2it [08:57, 268.72s/it]\nworker  - success: 0.879 - failed to download: 0.113 - failed to resize: 0.008 - images per sec: 39 - count: 10000\ntotal   - success: 0.875 - failed to download: 0.114 - failed to resize: 0.011 - images per sec: 38 - count: 20000\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_json('/kaggle/input/guie-laion5b-dataset/GUIE_laion5b_dataset_en.json')\ndf = df.loc[:, ['url', 'caption_en']]\ndf = df.iloc[:num_pairs_taken]\n\n# Building vocabulary\n\nPAD_token = 0   # Used for padding short sentences\nSOS_token = 1   # Start-of-sentence token\nEOS_token = 2   # End-of-sentence token\nCLS_token = 3   # Classification token\n\nclass Vocabulary():\n    def __init__(self, name):\n      self.name = name\n      self.word2index = {}\n      self.word2count = {}\n      self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", CLS_token: \"CLS\"}\n      self.num_words = 3\n      self.num_sentences = 0\n      self.longest_sentence = 0\n    \n    def add_word(self, word):\n      if word not in self.word2index:\n        # First entry of word into vocabulary\n        self.word2index[word] = self.num_words\n        self.word2count[word] = 1\n        self.index2word[self.num_words] = word\n        self.num_words += 1\n      else:\n        # Word exists; increase word count\n        self.word2count[word] += 1\n    \n    def add_sentence(self, sentence):\n      sentence_len = 0\n      for word in sentence.split(' '):\n        sentence_len += 1\n        self.add_word(word)\n        if sentence_len > self.longest_sentence:\n        # This is the longest sentence\n            self.longest_sentence = sentence_len\n      # Count the number of sentences\n        self.num_sentences += 1\n    \n    def to_word(self, index):\n      return self.index2word[index]\n\n    def to_index(self, word):\n      return self.word2index[word]\n\nclipVocab = Vocabulary('CLIP')\n\n\nfor ind in df.index:\n    clipVocab.add_sentence(df['caption_en'][ind])\n    \ndel df\nprint(f'{clipVocab.num_words} words obtained in Vocabulary')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:32:44.412328Z","iopub.execute_input":"2023-07-22T23:32:44.413037Z","iopub.status.idle":"2023-07-22T23:32:48.818663Z","shell.execute_reply.started":"2023-07-22T23:32:44.412985Z","shell.execute_reply":"2023-07-22T23:32:48.817633Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"36443 words obtained in Vocabulary\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_image_files_dict(base_path: Path) -> Dict:\n    image_files = [\n        *base_path.glob(\"**/*.png\"),\n        *base_path.glob(\"**/*.jpg\"),\n        *base_path.glob(\"**/*.jpeg\"),\n        *base_path.glob(\"**/*.bmp\"),\n    ]\n    return {image_file.stem: image_file for image_file in image_files}\n\n\ndef get_text_files_dict(base_path: Path) -> Dict:\n    text_files = [*base_path.glob(\"**/*.txt\")]\n    return {text_file.stem: text_file for text_file in text_files}\n\n\ndef get_shared_stems(image_files_dict: Dict, text_files_dict: Dict) -> List:\n    image_files_stems = set(image_files_dict.keys())\n    text_files_stems = set(text_files_dict.keys())\n    return list(image_files_stems & text_files_stems)\n\n\nclass TextImageDataset(Dataset):\n    \"\"\"Dataset for text-image pairs\"\"\"\n\n    def __init__(\n            self,\n            root: str,\n            preprocess: Callable = None,\n            tokenizer: Callable = None,\n    ):\n        super().__init__()\n        self.root = Path(root)\n        self.preprocess = preprocess\n        self.tokenizer = tokenizer\n\n        self.image_files_dict = get_image_files_dict(self.root)\n        self.text_files_dict = get_text_files_dict(self.root)\n        self.shared_stems = get_shared_stems(self.image_files_dict, self.text_files_dict)\n\n    def __len__(self):\n        return len(self.shared_stems)\n\n    def get_caption(self, text_file: Path):\n        with open(text_file, 'r') as f:\n            return f.read().strip()\n\n    def __getitem__(self, i: int):\n        stem = self.shared_stems[i]\n        # read image\n        image_file = self.image_files_dict[stem]\n        image = Image.open(image_file).convert(\"RGB\")\n\n        # read text\n        text_file = self.text_files_dict[stem]\n        text = self.get_caption(text_file)\n\n        # preprocess image and text\n        if self.preprocess:\n            image = self.preprocess(image)\n        \n        if self.tokenizer:\n            text = self.tokenizer(text)\n        return image, text\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:32:48.820192Z","iopub.execute_input":"2023-07-22T23:32:48.820801Z","iopub.status.idle":"2023-07-22T23:32:48.834638Z","shell.execute_reply.started":"2023-07-22T23:32:48.820763Z","shell.execute_reply":"2023-07-22T23:32:48.833640Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def make_patches(image_inp, num_of_patches):\n    n, c, h, w = image_inp.shape\n\n    patch_size_h = h//num_of_patches    \n    patch_size_w = w//num_of_patches    \n\n    patches = torch.zeros(n, num_of_patches**2, h * w * c // num_of_patches ** 2) \n    for idx, each_img in enumerate(image_inp):\n        for i in range(num_of_patches):\n            for j in range(num_of_patches):\n                patch = each_img[:, i*patch_size_h: (i+1)*patch_size_h, j*patch_size_w:(j+1)*patch_size_w]  \n                patches[idx, i*num_of_patches + j] = patch.flatten()\n\n    return patches\n\ndef get_positional_embeddings(sequence_length, depth):\n    out = torch.ones(sequence_length, depth)\n\n    for i in range(sequence_length):\n        for j in range(depth):\n            if j % 2 == 0:\n                out[i][j] = np.sin(i / (10_000 ** (j / depth)))\n            else:\n                out[i][j] = np.cos(i / (10_000 ** ((j - 1) / depth)))\n    return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:32:48.837323Z","iopub.execute_input":"2023-07-22T23:32:48.838346Z","iopub.status.idle":"2023-07-22T23:32:48.849551Z","shell.execute_reply.started":"2023-07-22T23:32:48.838311Z","shell.execute_reply":"2023-07-22T23:32:48.848440Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(pl.LightningModule):\n    \n    def __init__(self, num_heads, embed_size):\n        \"\"\"\n        Multi-Head Attention Layer that separates the embeddings into (num_heads) Heads and computes QKV attention\n        \n        Arguments:-\n        num_heads: Number of attention heads\n        embed_size: The input embedding dimension\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_size = embed_size \n        self.head_dim = embed_size // num_heads \n        \n        assert (self.head_dim * num_heads == embed_size), 'Embed size needs to be divisible by number of heads'\n        \n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n        self.fc_out = nn.Linear(self.head_dim * num_heads, embed_size)\n\n    def forward(self, values, keys, query, mask):\n        \"\"\"\n        Forward pass for our Multi-Head Attention layer\n        \n        Arguments:-\n        values: 'Same as below'\n        keys: 'Same as below'\n        query: (batch_dim, seq_length, seq_dimension) The inputted query matrix.\n        \"\"\"\n        N = values.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n        \n        values = values.reshape(N, value_len, self.num_heads, self.head_dim) \n        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n\n        values = self.values(values)\n        keys = self.values(keys)\n        query = self.queries(queries)\n        \n        values = values.transpose(1,2).contiguous().view(N*self.num_heads, value_len, self.head_dim)\n        keys = keys.transpose(1,2).contiguous().view(N*self.num_heads, key_len, self.head_dim) \n        queries = query.transpose(1,2).contiguous().view(N*self.num_heads, query_len, self.head_dim)\n        \n        energy = torch.bmm(queries, keys.transpose(1, 2)) \n        energy_scaled = energy / (self.embed_size ** 0.5)\n        \n        attention = F.softmax(energy_scaled, dim=2)\n        out = torch.bmm(attention, values).view(N, self.num_heads, query_len, self.head_dim)\n        out = out.transpose(1, 2).contiguous().view(N, query_len, self.head_dim * self.num_heads)\n        \n        return self.fc_out(out)\n    \nclass TransformerBlock(pl.LightningModule):\n    \n    def __init__(self, embed_dim, dropout, num_heads, forward_expansion):\n        \"\"\"\n        The Transformer Block is initialized with a few pre-defined parameters.\n        \n        Arguments:-\n        embed_dim: The embedding dimension for each token\n        dropout: The dropout rate\n        num_heads: Number of heads to create for Multi-Head Attention layer\n        forward_expansion: Arbitrary expansion dimension for the final feed-forward layer, post-attention (Check forward())\n        \"\"\"\n        super().__init__()\n        self.attention = SelfAttention(num_heads, embed_dim)\n        self.layer_norm1 = nn.LayerNorm(embed_dim)\n        self.layer_norm2 = nn.LayerNorm(embed_dim)\n        \n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * forward_expansion),\n            nn.ReLU(),\n            nn.Linear(embed_dim * forward_expansion, embed_dim)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, value, key, query, mask):\n        \"\"\"\n        Forward pass for the Transformer\n        \n        Arguments:-\n        value, key, query: (batch_dim, seq_length, seq_dimension) Inputted sequence, Usually the same\n        mask: Mask to fill in large -ve values so that the softmax in SelfAttention considers them as 0.\n        \"\"\"\n        attention = self.attention(value, key, query, mask)\n        x = self.dropout(self.layer_norm1(attention + query))\n        forward = self.feed_forward(x)\n        out = self.dropout(self.layer_norm2(forward + x))\n        \n        return out\n    \nclass Encoder(pl.LightningModule):\n    \n    def __init__(\n        self,\n        src_vocab_size,\n        embed_dim,\n        num_layers,\n        num_heads,\n        forward_expansion,\n        dropout,\n        max_length,\n        final_output_embed\n    ):\n        \"\"\"\n        The Encoder block is where the embeddings are formed, passed in through the Transformer Block for (num_layers) times, and\n        projected into a final embedding space\n        \n        Arguments:-\n        src_vocab_size: Total number of words in our Source Vocabulary\n        embed_dim: Words are embedded into embed_size\n        num_layers: Number of times to repeat transformer block\n        num_heads: Number of heads to create for Multi-Head Attention layer\n        #device: Deprecated for Lightning Implementation since lightning doesn't like device calls :D\n        forward_expansion: Arbitrary expansion dimension (Check Transformer Block)\n        dropout: Dropout rate\n        max_length: Maximum allowed sequence length, aids in getting positional embeddings\n        final_output_embed: Final embedding dimension after passing through Transformer Block\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.final_output_embed = final_output_embed\n        \n        self.word_embedding = nn.Embedding(src_vocab_size, embed_dim)\n        self.positional_embedding = nn.Embedding(max_length, embed_dim)\n        \n        \n        self.layers = nn.ModuleList(\n            [\n                TransformerBlock(\n                    embed_dim,\n                    dropout,\n                    num_heads,\n                    forward_expansion\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, self.final_output_embed)    \n        )\n    \n    def forward(self, inp, mask):\n        \"\"\"\n        Forward pass for the Encoder\n        \n        Arguments:-\n        inp: (num_samples, num_seq_length) Input for our Encoder\n        mask: The mask to pass to get masked attention\n        \"\"\"\n        \n\n        num_samples, seq_length = inp.shape\n        positions = torch.arange(0, seq_length).expand(num_samples, seq_length).to(device=self.device)\n        word_embed = self.word_embedding(inp)\n\n        pos_embed = self.positional_embedding(positions)\n        out = word_embed + pos_embed\n        \n        out = self.dropout(out)\n        for layer in self.layers:\n            out = layer(out, out, out, mask) \n            \n        out = self.mlp(out[:, 0]) # Taking the (final_text_embed) dimension CLS token\n        return out\n    \nclass Transformer(pl.LightningModule):\n    \n    def __init__(\n        self,\n        src_vocab_size,\n        src_pad_idx,\n        forward_expansion,\n        dropout: int=0,\n        embed_dim: int=768,\n        num_layers: int=6,\n        num_heads: int=8,\n        max_length: int=76,\n        final_output_embed: int=256\n    ):\n        \"\"\"\n        The Final Transformer Module that goes through our Encoder block with a mask and outputs the word_embeddings of dimension \n        (final_output_embed)\n        \n        Arguments:-\n        src_vocab_size: Total number of words in our Source Vocabulary\n        src_pad_idx: The index designated for <PAD> token\n        forward_expansion: Arbitrary expansion dimension (Check Transformer Block)\n        embed_dim: Words are embedded into embed_size\n        num_layers: Number of times to repeat transformer block\n        num_heads: Number of heads to create for Multi-Head Attention layer\n        #device: Deprecated for Lightning Implementation since lightning doesn't like device calls :D\n        dropout: Dropout rate\n        max_length: Maximum allowed sequence length, aids in getting positional embeddings\n        final_output_embed: Final embedding dimension after passing through Transformer Block\n        \"\"\"\n        super().__init__()\n        \n        self.encoder = Encoder(\n            src_vocab_size,\n            embed_dim,\n            num_layers,\n            num_heads,\n            forward_expansion,\n            dropout,\n            max_length,\n            final_output_embed\n        )\n\n        self.src_pad_idx = src_pad_idx\n        \n    def make_src_mask(self, src):\n        src_mask = (src == self.src_pad_idx).unsqueeze(1).unsqueeze(2)  # True if src == src_pad_idx, else False\n        return src_mask\n\n    def forward(self, src):\n        \"\"\"\n        Forward pass for the Transformer\n        \n        Arguments:-\n        src: Source Input (num_of_samples, max_seq_length [Before CLS])\n        \"\"\"\n        src = src.to(device=self.device)\n        n = src.shape[0]\n        tok = torch.full((n,1), 3).to(device=self.device)\n\n        src = torch.cat((tok, src), 1) # Adding a CLS token to the sentences\n        src_mask = self.make_src_mask(src) \n        enc_src = self.encoder(src, src_mask)      \n        return enc_src\n\nclass VisualTransformer(pl.LightningModule):\n    \n    def __init__(\n        self,\n        chw,\n        num_of_patches,\n        forward_expansion: int=4,\n        embed_dim: int=64,\n        num_layers: int=6,\n        num_heads: int=2,\n        dropout: int=0,\n        final_output_embed: int=256\n    ):\n        \"\"\"\n        The Visual Transformer patchifies our images into tokens and feeds them into a Transformer to get attentioned outputs\n        \n        Arguments:-\n        chw: Tuple of Channels, Height, Width \n        num_of_patches: Number of patches to divide the height and width into\n        forward_expansion: Arbitrary expansion dimension (Check Transformer Block)\n        embed_dim: Words are embedded into embed_size\n        num_layers: Number of times to repeat transformer block\n        num_heads: Number of heads to create for Multi-Head Attention layer\n        dropout: Dropout rate\n        final_output_embed: Final embedding dimension after passing through Transformer Block\n        \"\"\"\n        \n        super().__init__()\n\n        self.num_of_patches = num_of_patches\n        self.chw = chw\n        self.patch_size_h = self.chw[1] // num_of_patches\n        self.patch_size_w = self.chw[2] // num_of_patches\n        self.hidden_size = embed_dim  \n        self.final_output_embed = final_output_embed \n        \n        \n        self.fc1 = nn.Linear(self.chw[0] * self.patch_size_h * self.patch_size_w, self.hidden_size)\n\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_size))    \n        self.pos_embed = nn.Parameter(torch.Tensor(get_positional_embeddings(self.num_of_patches ** 2 + 1, self.hidden_size))) \n        self.pos_embed.requires_grad = False\n        \n        self.layers = nn.ModuleList(\n            [\n                TransformerBlock(\n                    embed_dim,\n                    dropout,\n                    num_heads,\n                    forward_expansion\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.mlp = nn.Sequential(\n                nn.Linear(self.hidden_size, self.final_output_embed)\n            )\n    \n    def forward(\n        self,\n        image_inp\n    ):\n\n        img_width = image_inp.shape[-1]\n        img_height = image_inp.shape[-2]\n        \n        assert img_width%self.num_of_patches == 0, 'Image width not divisible by number of patches'\n        assert img_height%self.num_of_patches == 0, 'Image height not divisible by number of patches'\n\n        patches = make_patches(image_inp, self.num_of_patches).to(device=self.device)\n        tokens = self.fc1(patches)  \n        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n        \n        positions = self.pos_embed.repeat(image_inp.shape[0], 1, 1)\n        tokens += positions\n        for layer in self.layers:\n            out = layer(tokens, tokens, tokens, None) \n\n        out = self.mlp(out[:, 0])\n        return out \n\nclass CLIP(pl.LightningModule):\n    \n    def __init__(\n        self,\n        chw, \n        num_of_patches,\n        src_vocab_size,\n        device,\n        src_pad_idx: int=0,\n        forward_expansion: int=4\n    ):\n        super().__init__()\n        self.visual_transformer = VisualTransformer(\n            chw=chw, \n            num_of_patches=num_of_patches\n        )\n        self.text_transformer = Transformer(\n            src_vocab_size,\n            src_pad_idx,\n            forward_expansion\n        )\n        self.maxie = 75\n        print(f'We are now on {self.device}')\n        \n    \n    def training_step(self, batch, batch_idx):\n        img, cap = batch\n        sents = [each_sent.split() for each_sent in cap]\n        max_seq_length = self.maxie\n        temperature = 0.07\n        \n        try:\n            tokens = [[1] + [clipVocab.to_index(word) for word in sent] + [2] for sent in sents]\n        except:\n            print(f'Key error in tokenizer! Skipped batch number {batch_idx} :(')\n            return None\n\n        for idx, token in enumerate(tokens):\n            if len(token) >= max_seq_length:\n                token = token[:max_seq_length]\n                token[-1] = 2\n                tokens[idx] = torch.Tensor(token)\n            if len(token) < max_seq_length:\n                diff = max_seq_length - len(token)\n                token = token + [0]*diff\n                tokens[idx] = torch.Tensor(token)\n        \n        tokens = torch.vstack(tokens).to(torch.int64).to(device=self.device)\n        img = img.to(device=self.device)\n        \n        I_f = self.visual_transformer(img)\n        T_f = self.text_transformer(tokens)\n        \n        logits = ((T_f @ I_f.T) * np.exp(temperature))\n\n        targets = torch.eye(logits.shape[0]).to(device=self.device)\n        loss = F.cross_entropy(logits, targets)\n        \n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n       \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr = 0.001)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T00:01:35.511268Z","iopub.execute_input":"2023-07-23T00:01:35.516139Z","iopub.status.idle":"2023-07-23T00:01:35.726873Z","shell.execute_reply.started":"2023-07-23T00:01:35.516094Z","shell.execute_reply":"2023-07-23T00:01:35.725673Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"class ImgCapDataModule(pl.LightningDataModule):\n    \n    def __init__(self, data_dir, batch_size: int = 64, num_workers: int = 4):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = 64\n        self.num_workers = num_workers\n        \n    def setup(self, stage):\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor()\n        ])\n        \n        self.train_ds = TextImageDataset(out_dir, self.preprocess)\n        \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_ds,\n            batch_size=64,\n            num_workers=self.num_workers,\n            shuffle=False\n        )","metadata":{"execution":{"iopub.status.busy":"2023-07-23T00:01:35.729639Z","iopub.execute_input":"2023-07-23T00:01:35.729992Z","iopub.status.idle":"2023-07-23T00:01:35.758026Z","shell.execute_reply.started":"2023-07-23T00:01:35.729957Z","shell.execute_reply":"2023-07-23T00:01:35.754435Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#Training config\nchw = (3, 224, 224)\nnum_of_patches = 8\nsrc_vocab_size = clipVocab.num_words\nbatch_size=64\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-07-23T00:01:35.772827Z","iopub.execute_input":"2023-07-23T00:01:35.775359Z","iopub.status.idle":"2023-07-23T00:01:35.797232Z","shell.execute_reply.started":"2023-07-23T00:01:35.775317Z","shell.execute_reply":"2023-07-23T00:01:35.796208Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"clipModel = CLIP(chw=chw, num_of_patches=num_of_patches, src_vocab_size=src_vocab_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:56:30.027563Z","iopub.execute_input":"2023-07-22T23:56:30.028161Z","iopub.status.idle":"2023-07-22T23:56:30.634461Z","shell.execute_reply.started":"2023-07-22T23:56:30.028126Z","shell.execute_reply":"2023-07-22T23:56:30.633289Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"We are now on cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"dm = ImgCapDataModule(data_dir=\"dataset/\", batch_size=64)\ntrainer = pl.Trainer(accelerator='gpu', min_epochs=2, max_epochs=32)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:56:30.636188Z","iopub.execute_input":"2023-07-22T23:56:30.636591Z","iopub.status.idle":"2023-07-22T23:56:30.711651Z","shell.execute_reply.started":"2023-07-22T23:56:30.636553Z","shell.execute_reply":"2023-07-22T23:56:30.710736Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"trainer.fit(clipModel, dm)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T23:56:30.713199Z","iopub.execute_input":"2023-07-22T23:56:30.713569Z","iopub.status.idle":"2023-07-23T00:01:35.504741Z","shell.execute_reply.started":"2023-07-22T23:56:30.713535Z","shell.execute_reply":"2023-07-23T00:01:35.503246Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d6a2d9e92e42fdb14cc884773f03b9"}},"metadata":{}},{"name":"stdout","text":"Key error in tokenizer! Skipped batch number 47 :(\nKey error in tokenizer! Skipped batch number 49 :(\nKey error in tokenizer! Skipped batch number 64 :(\nKey error in tokenizer! Skipped batch number 95 :(\nKey error in tokenizer! Skipped batch number 98 :(\nKey error in tokenizer! Skipped batch number 109 :(\nKey error in tokenizer! Skipped batch number 132 :(\nKey error in tokenizer! Skipped batch number 255 :(\nKey error in tokenizer! Skipped batch number 258 :(\nKey error in tokenizer! Skipped batch number 47 :(\nKey error in tokenizer! Skipped batch number 49 :(\nKey error in tokenizer! Skipped batch number 64 :(\nKey error in tokenizer! Skipped batch number 95 :(\nKey error in tokenizer! Skipped batch number 98 :(\nKey error in tokenizer! Skipped batch number 109 :(\nKey error in tokenizer! Skipped batch number 132 :(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}