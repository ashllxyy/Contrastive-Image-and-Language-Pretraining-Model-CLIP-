{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install img2dataset","metadata":{"_uuid":"90f888a7-6524-4d75-9b23-0c928c2af181","_cell_guid":"505de7aa-fccb-4b1b-903b-4f3d87ccc23e","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:26:25.739022Z","iopub.execute_input":"2023-07-21T04:26:25.739519Z","iopub.status.idle":"2023-07-21T04:27:21.138296Z","shell.execute_reply.started":"2023-07-21T04:26:25.739476Z","shell.execute_reply":"2023-07-21T04:27:21.137086Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_json = '../input/guie-laion5b-dataset/GUIE_laion5b_dataset_en.json'\n!ls $path_json","metadata":{"_uuid":"bd60d8a9-8ed2-47c0-9e71-6247fa884314","_cell_guid":"ad863ee5-510d-4013-8c69-38dd00458669","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:27:21.143019Z","iopub.execute_input":"2023-07-21T04:27:21.143344Z","iopub.status.idle":"2023-07-21T04:27:22.154939Z","shell.execute_reply.started":"2023-07-21T04:27:21.143312Z","shell.execute_reply":"2023-07-21T04:27:22.153737Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Dict, List, Callable\nfrom pathlib import Path\nfrom PIL import Image\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms","metadata":{"_uuid":"da800861-03f5-4eb9-99b3-4d76f57f75c8","_cell_guid":"47004cf0-47a0-4f26-a4ce-8376919ca061","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:27:22.158663Z","iopub.execute_input":"2023-07-21T04:27:22.159010Z","iopub.status.idle":"2023-07-21T04:27:24.121150Z","shell.execute_reply.started":"2023-07-21T04:27:22.158977Z","shell.execute_reply":"2023-07-21T04:27:24.120081Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(path_json)\ndf['text'] = df['caption']\ndf['text_en'] = df['caption_en']\n\ndf.drop(columns=['caption', 'caption_en'], inplace=True)\n\ndf_head = df.head(500)\ndf_head.to_json('./fixed_df.json', orient='records')","metadata":{"_uuid":"700356d4-b321-48a4-bfcf-9fb0d0005ef2","_cell_guid":"d79cc63a-6a68-44dc-a460-4384a23bc4fc","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:27:24.123973Z","iopub.execute_input":"2023-07-21T04:27:24.124741Z","iopub.status.idle":"2023-07-21T04:27:30.492320Z","shell.execute_reply.started":"2023-07-21T04:27:24.124702Z","shell.execute_reply":"2023-07-21T04:27:30.491144Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_dir = Path('./dataset')\nout_dir.mkdir(exist_ok=True)\n\n!img2dataset \"fixed_df.json\" --input_format=\"json\" --caption_col=\"text_en\" --output_folder=$out_dir --processes_count=1 --output_format=\"files\" --resize_mode=\"no\"","metadata":{"_uuid":"a9558f5a-a719-4064-80a3-9e4bce0b2f19","_cell_guid":"9e09bcff-c7e3-44b0-8faa-58ce21e85db1","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:27:30.494040Z","iopub.execute_input":"2023-07-21T04:27:30.494391Z","iopub.status.idle":"2023-07-21T04:28:06.869164Z","shell.execute_reply.started":"2023-07-21T04:27:30.494357Z","shell.execute_reply":"2023-07-21T04:28:06.867876Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_files_dict(base_path: Path) -> Dict:\n    image_files = [\n        *base_path.glob(\"**/*.png\"),\n        *base_path.glob(\"**/*.jpg\"),\n        *base_path.glob(\"**/*.jpeg\"),\n        *base_path.glob(\"**/*.bmp\"),\n    ]\n    return {image_file.stem: image_file for image_file in image_files}\n\n\ndef get_text_files_dict(base_path: Path) -> Dict:\n    text_files = [*base_path.glob(\"**/*.txt\")]\n    return {text_file.stem: text_file for text_file in text_files}\n\n\ndef get_shared_stems(image_files_dict: Dict, text_files_dict: Dict) -> List:\n    image_files_stems = set(image_files_dict.keys())\n    text_files_stems = set(text_files_dict.keys())\n    return list(image_files_stems & text_files_stems)\n\n\nclass TextImageDataset(Dataset):\n    \"\"\"Dataset for text-image pairs\"\"\"\n\n    def __init__(\n            self,\n            root: str,\n            preprocess: Callable = None,\n            tokenizer: Callable = None,\n    ):\n        super().__init__()\n        self.root = Path(root)\n        self.preprocess = preprocess\n        self.tokenizer = tokenizer\n\n        self.image_files_dict = get_image_files_dict(self.root)\n        self.text_files_dict = get_text_files_dict(self.root)\n        self.shared_stems = get_shared_stems(self.image_files_dict, self.text_files_dict)\n\n    def __len__(self):\n        return len(self.shared_stems)\n\n    def get_caption(self, text_file: Path):\n        with open(text_file, 'r') as f:\n            return f.read().strip()\n\n    def __getitem__(self, i: int):\n        stem = self.shared_stems[i]\n        # read image\n        image_file = self.image_files_dict[stem]\n        image = Image.open(image_file).convert(\"RGB\")\n\n        # read text\n        text_file = self.text_files_dict[stem]\n        text = self.get_caption(text_file)\n\n        # preprocess image and text\n        if self.preprocess:\n            image = self.preprocess(image)\n        \n        if self.tokenizer:\n            text = self.tokenizer(text)\n        return image, text","metadata":{"_uuid":"cd359eed-edab-466b-b8a7-5511c7fb4811","_cell_guid":"5529b3cc-675f-4dcf-9282-c52703d93000","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:06.871226Z","iopub.execute_input":"2023-07-21T04:28:06.871654Z","iopub.status.idle":"2023-07-21T04:28:06.887680Z","shell.execute_reply.started":"2023-07-21T04:28:06.871613Z","shell.execute_reply":"2023-07-21T04:28:06.886271Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor()\n])\n\ndataset = TextImageDataset(out_dir, trans)\nfor i,_ in enumerate(dataset):\n    print(i)\n    break","metadata":{"_uuid":"706483d1-116f-47eb-91af-2b4919562485","_cell_guid":"f4863b79-b0c5-480c-a609-0903c5e9c360","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:06.889383Z","iopub.execute_input":"2023-07-21T04:28:06.889802Z","iopub.status.idle":"2023-07-21T04:28:07.047141Z","shell.execute_reply.started":"2023-07-21T04:28:06.889765Z","shell.execute_reply":"2023-07-21T04:28:07.045850Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, caption = dataset[16]\n\nplt.figure()\nplt.title(caption)\nplt.imshow(img.numpy().transpose(1, 2, 0))\nplt.axis('off')","metadata":{"_uuid":"00e2b27a-ba83-42a9-9741-93e164fc70e3","_cell_guid":"23e88e70-628a-4fde-9825-dcaf6feda7cd","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:07.050253Z","iopub.execute_input":"2023-07-21T04:28:07.051157Z","iopub.status.idle":"2023-07-21T04:28:07.416347Z","shell.execute_reply.started":"2023-07-21T04:28:07.051122Z","shell.execute_reply":"2023-07-21T04:28:07.415346Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=16)\n\nfor idx,(img, caption) in enumerate(dataloader):\n    print(f'For {idx}', img.shape, caption, end='\\n\\n')\n    break\n# #   img = img.squeeze().numpy().transpose(1, 2, 0)\n#     print(img.shape)\n#     caption = caption[0]\n    \n#     plt.figure()\n#     plt.title(caption)\n#     plt.imshow(img)\n#     plt.show()\n#     break","metadata":{"_uuid":"30c03bba-dce4-46ed-8ad4-d373b20583f3","_cell_guid":"17bc7117-3b0b-4729-9154-97fb6803f30e","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:07.417418Z","iopub.execute_input":"2023-07-21T04:28:07.417779Z","iopub.status.idle":"2023-07-21T04:28:07.558825Z","shell.execute_reply.started":"2023-07-21T04:28:07.417746Z","shell.execute_reply":"2023-07-21T04:28:07.555861Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport requests\nimport io\nimport time\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"_uuid":"836d88bb-7c7e-4948-9b68-18920b865362","_cell_guid":"a35f9472-1edc-4977-a57e-0533b657d776","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:07.565877Z","iopub.execute_input":"2023-07-21T04:28:07.566734Z","iopub.status.idle":"2023-07-21T04:28:07.573247Z","shell.execute_reply.started":"2023-07-21T04:28:07.566674Z","shell.execute_reply":"2023-07-21T04:28:07.572218Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json('/kaggle/input/guie-laion5b-dataset/GUIE_laion5b_dataset_en.json')\ndf = df.loc[:, ['url', 'caption_en']]\ndf = df.iloc[:20_000]\n\n# Building vocabulary\n\nPAD_token = 0   # Used for padding short sentences\nSOS_token = 1   # Start-of-sentence token\nEOS_token = 2   # End-of-sentence token\nCLS_token = 3\n\nclass Vocabulary():\n    def __init__(self, name):\n      self.name = name\n      self.word2index = {}\n      self.word2count = {}\n      self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", CLS_token: \"CLS\"}\n      self.num_words = 3\n      self.num_sentences = 0\n      self.longest_sentence = 0\n    \n    def add_word(self, word):\n      if word not in self.word2index:\n        # First entry of word into vocabulary\n        self.word2index[word] = self.num_words\n        self.word2count[word] = 1\n        self.index2word[self.num_words] = word\n        self.num_words += 1\n      else:\n        # Word exists; increase word count\n        self.word2count[word] += 1\n    \n    def add_sentence(self, sentence):\n      sentence_len = 0\n      for word in sentence.split(' '):\n        sentence_len += 1\n        self.add_word(word)\n        if sentence_len > self.longest_sentence:\n        # This is the longest sentence\n            self.longest_sentence = sentence_len\n      # Count the number of sentences\n        self.num_sentences += 1\n    \n    def to_word(self, index):\n      return self.index2word[index]\n\n    def to_index(self, word):\n      return self.word2index[word]\n\n\nclipVocab = Vocabulary('CLIP')","metadata":{"_uuid":"8bcf794f-2858-4d56-aa0d-7059e6b81cb3","_cell_guid":"d1f75f02-aa88-416f-8398-aa0ce7a45ab8","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:07.574787Z","iopub.execute_input":"2023-07-21T04:28:07.575474Z","iopub.status.idle":"2023-07-21T04:28:11.686339Z","shell.execute_reply.started":"2023-07-21T04:28:07.575437Z","shell.execute_reply":"2023-07-21T04:28:11.685228Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ind in df.index:\n    clipVocab.add_sentence(df['caption_en'][ind])\n    if ind%5000 == 0:\n        print(f'Reached {ind}')\ndel df\nprint(f'{clipVocab.num_words} in Vocabulary')","metadata":{"_uuid":"b5100c54-220c-4416-89ec-71a2ccc7a2a3","_cell_guid":"fc2fb7c0-7132-4f37-8149-4dfd37de67d4","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:11.688112Z","iopub.execute_input":"2023-07-21T04:28:11.688560Z","iopub.status.idle":"2023-07-21T04:28:12.197689Z","shell.execute_reply.started":"2023-07-21T04:28:11.688500Z","shell.execute_reply":"2023-07-21T04:28:12.196528Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n  def __init__(self, embed_size, heads):\n    super().__init__()\n    self.embed_size = embed_size\n    self.heads = heads\n    self.head_dim = embed_size // heads\n\n    assert (self.head_dim * heads == embed_size), 'Embed size needs to be divisible by number of heads'\n\n    # Each head will get keys, values and queries\n    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n    self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n\n  def forward(self, values, keys, query, mask):\n    N = query.shape[0] # num of training samples\n\n    # these vectors will be of same length as source/target sentence\n    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n    values = values.reshape(N, value_len, self.heads, self.head_dim)\n    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n\n    values = self.values(values)\n    keys = self.keys(keys)\n    queries = self.queries(queries)\n\n    energy = torch.einsum('nqhd,nkhd->nhqk', [queries, keys])\n    # query shape: [num_of_samples, query_len, heads, heads_dim]\n    # keys shape: [num_of_samples, key_len, heads, heads_dim]\n    # energy shape: [N, heads, query_len, key_len]\n\n    if mask is not None:\n        energy = energy.masked_fill(mask, float(\"-1e28\"))\n\n    attention = torch.softmax(energy / (self.embed_size ** (0.5)), dim = 3)\n\n    out = torch.einsum('nhql,nlhd->nqhd', [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n    # attention shape: (N, heads, query_len, key_len)\n    # values shape: (N, value_len, heads, heads_dim)\n    # (N, query_len, heads, head_dim)\n\n    out = self.fc_out(out)\n    return out\n\n\nclass TransformerBlock(nn.Module):\n  def __init__(self, embed_size, heads, dropout, forward_expansion):\n    super().__init__()\n    self.attention = SelfAttention(embed_size, heads)\n    self.norm1 = nn.LayerNorm(embed_size)\n    self.norm2 = nn.LayerNorm(embed_size)\n\n    self.feed_forward = nn.Sequential(\n        nn.Linear(embed_size, forward_expansion * embed_size),\n        nn.ReLU(),\n        nn.Linear(forward_expansion*embed_size, embed_size)\n    )\n\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, value, key, query, mask):\n\n    attention = self.attention(value, key, query, mask)\n\n    x = self.dropout(self.norm1(attention + query))\n    forward = self.feed_forward(x)\n    out = self.dropout(self.norm2(forward + x))\n    return out\n\nclass Encoder(nn.Module):\n  def __init__(\n      self,\n      src_vocab_size,\n      embed_size,\n      num_layers,\n      heads,\n      device,\n      forward_expansion,\n      dropout,\n      max_length\n  ):\n\n    super().__init__()\n    self.embed_size = embed_size\n    self.device = device\n    \n   \n    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n    self.positional_embedding = nn.Embedding(max_length, embed_size)\n\n    self.layers = nn.ModuleList(\n        [\n            TransformerBlock(\n                embed_size,\n                heads,\n                dropout=dropout,\n                forward_expansion=forward_expansion\n            )\n          for _ in range(num_layers)\n        ]\n    )\n    self.final_text_embed = 256\n    self.dropout = nn.Dropout(dropout)\n    self.mlp = nn.Sequential(\n        nn.Linear(embed_size, self.final_text_embed)    # Only using linear because we handle softmax outside the transformer\n    )\n\n  def forward(self, x, mask):\n    N, seq_length = x.shape     # (num_samples, num_seq_length)\n    # print(f'The shape of forward in Encoder input is {x.shape}')\n    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n    word_embed = self.word_embedding(x)     # (7, 21, 256)\n    pos_embed = self.positional_embedding(positions)     # (7, 21, 256)\n    out = word_embed + pos_embed\n\n    out = self.dropout(out)\n\n    for layer in self.layers:\n      out = layer(out, out, out, mask) # Passing out into the key, query and value of the transformer block\n\n    out = self.mlp(out[:, 0])   # Taking the 256 dimension CLS token from all the sentences and putting it into an MLP\n    # print(f'Shape of out is {out.shape}')\n    return out      # (7, 256)\n\nclass ViT(nn.Module):\n  def __init__(\n    self,\n    chw,\n    num_of_patches,\n    forward_expansion=4,\n    embed_size=64,\n    num_layers=6,\n    heads=2,\n    dropout=0,\n    device='cuda'\n  ):\n\n    super().__init__()\n\n    self.num_of_patches = num_of_patches\n    self.chw = chw\n    self.patch_size_h = self.chw[1]//num_of_patches\n    self.patch_size_w = self.chw[2]//num_of_patches\n    self.hidden_size = embed_size   # 8\n\n    self.fc1 = nn.Linear(self.chw[0] * self.patch_size_h * self.patch_size_w, self.hidden_size)\n\n    self.class_token = nn.Parameter(torch.rand(1, self.hidden_size))    # Random tensor of shape (1, hidden-size -> 8)\n    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.num_of_patches ** 2 + 1, self.hidden_size)))  # Function is along with make_patches()\n    self.pos_embed.requires_grad = False\n\n    self.layers = nn.ModuleList(\n        [\n            TransformerBlock(\n                embed_size,\n                heads,\n                dropout=dropout,\n                forward_expansion=forward_expansion\n            )\n          for _ in range(num_layers)\n        ]\n    )\n    self.final_out = 256 # Could be number of samples of image-text pairs------\n    self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_size, self.final_out),\n            # nn.Softmax(dim=-1) -- Removing the softmax\n        )\n  def forward(\n    self,\n    image_inp\n  ):\n\n    img_width = image_inp.shape[-1]\n    img_height = image_inp.shape[-2]\n    assert img_width%self.num_of_patches == 0, 'Image width not divisible by number of patches'\n    assert img_height%self.num_of_patches == 0, 'Image height not divisible by number of patches'\n\n    patches = make_patches(image_inp, self.num_of_patches).to(device=device) # (num_of_imgs, num_of_patches, patch_h * patch_w * num_channels)\n    tokens = self.fc1(patches)  # patch_h * patch_w * num_channels -> hidden_size\n\n    # [num_of_images, num_of_patches, hidden_size(size we reduced to)]\n    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n    # A class token is added to each patch, to all 8 dims simultaenously\n    # [num_of_images, num_of_patches + 1 (class token), hidden_size]\n\n    positions = self.pos_embed.repeat(image_inp.shape[0], 1, 1)\n    tokens += positions\n    for layer in self.layers:\n       out = layer(tokens, tokens, tokens, None)  # -> Putting tokens in query, key and value of transformer block ------- # What to do with mask -> We don't use masking in CLIP\n\n    out = self.mlp(out[:, 0])  # torch.Size([7, 101, 64]) -> Taking the 64 dimension CLS token of all the images and passing it through MLP\n    return out  # (7, 256)     # We could have either taken the CLS token or performed global average pooling","metadata":{"_uuid":"3d137a1b-84d0-4e1b-90ba-3239090bd1bd","_cell_guid":"d4150711-f129-4f42-b231-e9745ddd75db","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:12.200209Z","iopub.execute_input":"2023-07-21T04:28:12.200611Z","iopub.status.idle":"2023-07-21T04:28:12.234827Z","shell.execute_reply.started":"2023-07-21T04:28:12.200572Z","shell.execute_reply":"2023-07-21T04:28:12.233788Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipTransformer(nn.Module):\n  def __init__(\n      self,\n      src_vocab_size,\n      src_pad_idx,\n      forward_expansion,\n      embed_size=768,\n      num_layers=6,\n      heads=8,\n      dropout=0,\n      device= 'cuda',\n      max_length=76\n  ):\n    super().__init__()\n\n    self.encoder = Encoder(\n        src_vocab_size,\n        embed_size,\n        num_layers,\n        heads,\n        device,\n        forward_expansion,\n        dropout,\n        max_length\n    )\n\n    self.src_pad_idx = src_pad_idx\n    self.device = device\n\n  def make_src_mask(self, src):\n\n    src_mask = (src == self.src_pad_idx).unsqueeze(1).unsqueeze(2)  # True wherever there's a masked index, false everywhere else\n    return src_mask.to(self.device)\n\n  def forward(self, src):\n    src = src.to(device=device) # # (num_of_samples, max_seq_length (before CLS token))\n    n = src.shape[0]\n    tok = torch.full((n,1), 3).to(device=device)\n    \n    src = torch.cat((tok, src), 1) # Adding a CLS token to the sentences\n    src_mask = self.make_src_mask(src)  # Boolean mask that has True wherever there's a value and False wherever there's the pad-index value, i.e 0 in our case\n    enc_src = self.encoder(src, src_mask)       # (num_of_samples, hidden_dim)\n    return enc_src","metadata":{"_uuid":"6854d6cf-9db0-4296-ad70-8636aa557eff","_cell_guid":"ca37bb52-27fa-423a-962b-1c2c107cdcdd","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:12.238299Z","iopub.execute_input":"2023-07-21T04:28:12.239040Z","iopub.status.idle":"2023-07-21T04:28:12.253357Z","shell.execute_reply.started":"2023-07-21T04:28:12.238990Z","shell.execute_reply":"2023-07-21T04:28:12.252428Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_patches(image_inp, num_of_patches):\n    n, c, h, w = image_inp.shape\n    # 2, 1, 50, 100\n    patch_size_h = h//num_of_patches    # 50/10 = 5\n    patch_size_w = w//num_of_patches    # 100/10 = 10\n\n    patches = torch.zeros(n, num_of_patches**2, h * w * c // num_of_patches ** 2) # (N, patches, patch dimensionality)      # (2, 100, 50)\n    for idx, each_img in enumerate(image_inp):\n      for i in range(num_of_patches):\n        for j in range(num_of_patches):\n          patch = each_img[:, i*patch_size_h: (i+1)*patch_size_h, j*patch_size_w:(j+1)*patch_size_w]  # (channels, width, height)   # patch of (1, 0:5, 10:20)\n          patches[idx, i*num_of_patches + j] = patch.flatten()\n    return patches\n\ndef get_positional_embeddings(sequence_length, depth):\n    out = torch.ones(sequence_length, depth)\n\n    for i in range(sequence_length):\n        for j in range(depth):\n          if j % 2 == 0:\n            out[i][j] = np.sin(i / (10000 ** (j / depth)))\n          else:\n            out[i][j] = np.cos(i / (10000 ** ((j - 1) / depth)))\n    return out","metadata":{"_uuid":"f00e2c11-b8d1-4e7f-ad5c-126a8f01ba5b","_cell_guid":"8423fc8f-8b67-428c-987d-7681c54301e3","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:28:12.255467Z","iopub.execute_input":"2023-07-21T04:28:12.255844Z","iopub.status.idle":"2023-07-21T04:28:12.273011Z","shell.execute_reply.started":"2023-07-21T04:28:12.255817Z","shell.execute_reply":"2023-07-21T04:28:12.272018Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['TORCH_USE_CUDA_DSA'] = '1'","metadata":{"_uuid":"9275e44e-e9b4-4b26-8c61-88a87dad2dad","_cell_guid":"6e71d82a-fe81-4d51-ac02-e0de7512aaf8","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:53:16.547372Z","iopub.execute_input":"2023-07-21T04:53:16.548697Z","iopub.status.idle":"2023-07-21T04:53:16.554123Z","shell.execute_reply.started":"2023-07-21T04:53:16.548646Z","shell.execute_reply":"2023-07-21T04:53:16.552871Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(clipVocab.num_words)\nmodel1 = ViT((3, 224, 224), 8).to(device=device)   # (chw, num_of_patches)\nmodel2 = ClipTransformer(clipVocab.num_words, 0, 4).to(device=device)   # (Vocab_size, pad_idx, forward_expansion)\ndevice","metadata":{"_uuid":"90c24acb-f525-4eaf-8e81-54ac70e5e222","_cell_guid":"6e56e840-eca0-40d7-ad77-0c70dd2632c3","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:53:16.809641Z","iopub.execute_input":"2023-07-21T04:53:16.810260Z","iopub.status.idle":"2023-07-21T04:53:17.485750Z","shell.execute_reply.started":"2023-07-21T04:53:16.810205Z","shell.execute_reply":"2023-07-21T04:53:17.484571Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = Adam([\n    {'params': model1.parameters()},\n    {'params': model2.parameters()}\n])","metadata":{"_uuid":"07fe8d24-1d9c-4323-97e0-3c7ddc98b04d","_cell_guid":"c51cb7cd-92fc-45e3-9ead-953f26e441c4","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:53:17.839051Z","iopub.execute_input":"2023-07-21T04:53:17.839553Z","iopub.status.idle":"2023-07-21T04:53:17.853172Z","shell.execute_reply.started":"2023-07-21T04:53:17.839485Z","shell.execute_reply":"2023-07-21T04:53:17.851396Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():                   \n    torch.cuda.empty_cache()","metadata":{"_uuid":"96307f1a-d850-4f35-a8f4-469e657f9d7d","_cell_guid":"46b65aaf-82f2-49a2-af61-1afa44b84bab","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:52:25.910425Z","iopub.execute_input":"2023-07-21T04:52:25.911080Z","iopub.status.idle":"2023-07-21T04:52:38.072332Z","shell.execute_reply.started":"2023-07-21T04:52:25.911044Z","shell.execute_reply":"2023-07-21T04:52:38.071014Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 32\nmax_seq_length = 75\ntemperature = 0.07\nT_max = 32\n\nfor epoch in range(num_epochs):\n    for batch_idx, (img, captions) in enumerate(dataloader):\n        sent = [i.split() for i in captions]\n        \n        try:\n            tokens = [[1] + [clipVocab.to_index(word) for word in each_sent] + [2] for each_sent in sent]\n        except:\n            continue\n        \n        for idx, token in enumerate(tokens):\n            if len(token) >= max_seq_length:\n                token = token[:max_seq_length]\n                token[-1] = 2\n                tokens[idx] = torch.Tensor(token)\n            if len(token) < max_seq_length:\n                diff = max_seq_length - len(token)\n                token = token + [0]*diff\n                tokens[idx] = torch.Tensor(token)\n        \n        tokens = torch.vstack(tokens).to(torch.int64).to(device=device)\n        img = img.to(device=device)\n        \n        I_f = model1(img)\n        T_f = model2(tokens)\n        \n        logits = ((T_f @ I_f.T) * np.exp(temperature)).to(device=device)\n        \n        img_t = I_f @ I_f.T\n        text_t = T_f @ T_f.T\n        \n        targets = F.softmax(\n            (img_t + text_t) / 2 * np.exp(temperature), dim=-1\n        )\n        print('This is what we have \\n', softs, 'This is what we\\'re going for', targets)\n#         targets = torch.eye(logits.shape[0]).to(device=device)\n        softs = F.softmax(logits, dim=1)\n        \n        img_loss = criterion(logits, targets)\n        text_loss = criterion(logits.T, targets.T)\n        \n        loss = (img_loss + text_loss)/2.0\n        loss = loss.mean()\n        \n        optimizer.zero_grad()\n        print(f'Loss: {loss}')\n        loss.backward()\n        optimizer.step()\n        \n        free_gpu_cache()\n        break\n    print(f'Reached epoch: {epoch+1}/{num_epochs}')","metadata":{"_uuid":"366dd997-d49f-459b-9b21-090495e8e4fe","_cell_guid":"8fe11f50-e7d0-4407-990e-fddc45043e8a","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T04:53:20.898734Z","iopub.execute_input":"2023-07-21T04:53:20.899138Z","iopub.status.idle":"2023-07-21T04:53:23.812640Z","shell.execute_reply.started":"2023-07-21T04:53:20.899098Z","shell.execute_reply":"2023-07-21T04:53:23.810868Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_img_matches(inp_sent):\n    sent = inp_sent.split()\n    max_seq_length = 75\n    \n    dataloader = DataLoader(dataset, batch_size=16)\n    try:\n        tokens = [1] + [clipVocab.to_index(word) for word in sent] + [2]\n    except:\n        print('Key error!')\n        return\n    \n    if len(tokens) >= max_seq_length:\n        tokens = tokens[:max_seq_length]\n        tokens[-1] = 2\n    if len(tokens) < max_seq_length:\n        diff = max_seq_length - len(tokens)\n        tokens = tokens + [0]*diff\n    \n    tokens = torch.Tensor(tokens).unsqueeze(0).to(torch.int64).to(device=device)\n    \n    best_img = -1\n    highest_score = 0\n    T_f = model2(tokens)\n    \n    for idx, (img, caption) in enumerate(dataloader):\n        with torch.no_grad():\n            I_f = model1(img)\n            logits = ((I_f @ T_f.T) * np.exp(temperature)).to(device=device)\n            softs = F.softmax(logits, dim=0)\n            if torch.max(logits[0]) > highest_score:\n                print(f'Got match at {idx*16 + torch.argmax(logits[0])}')\n                highest_score = torch.max(logits[0])\n                best_img = torch.argmax(logits[0])\n                img_prev = img[best_img].squeeze().numpy().transpose(1, 2, 0)\n                plt.figure()\n                plt.title(caption)\n                plt.imshow(img_prev)\n                plt.show()","metadata":{"_uuid":"76206bc4-0ac7-416c-9787-9ba5a15d2f76","_cell_guid":"7eca813a-2fe9-4f96-b533-4f029147f94d","collapsed":false,"execution":{"iopub.status.busy":"2023-07-21T03:36:05.622873Z","iopub.execute_input":"2023-07-21T03:36:05.623261Z","iopub.status.idle":"2023-07-21T03:36:05.635261Z","shell.execute_reply.started":"2023-07-21T03:36:05.623228Z","shell.execute_reply":"2023-07-21T03:36:05.633691Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_img_matches('Gladiator')","metadata":{"_uuid":"f5910682-42d2-45f0-a684-2ad466301132","_cell_guid":"7d0fd772-8f75-4e44-9438-0a56308e9506","collapsed":false,"execution":{"iopub.status.busy":"2023-07-19T16:38:16.730912Z","iopub.status.idle":"2023-07-19T16:38:16.731700Z","shell.execute_reply.started":"2023-07-19T16:38:16.731462Z","shell.execute_reply":"2023-07-19T16:38:16.731485Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"bc3d18aa-1741-483a-8546-0d7755c51711","_cell_guid":"b48dae69-49dd-4837-be6c-84986157ad30","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}